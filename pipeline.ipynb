{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.reader import read_data_from_url, extract_relevant_data, shuffle_lists_together, split_graphs_dataset\n",
    "from scripts.plotter import plot_distributions, plot_training_curves\n",
    "from scripts.dataset import prepare_graph_dataset, load_into_dataloader\n",
    "from scripts.predictor import compute_metrics_and_matrix_classification\n",
    "from scripts.trainer import train_model, build_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = read_data_from_url()\n",
    "data_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data & shuffle the lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_data, smile_data, pIC50_data = extract_relevant_data(data_frame=data_frame)\n",
    "names_data, smile_data, pIC50_data = shuffle_lists_together(names_data, smile_data, pIC50_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the datasets into train, valid & infer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = split_graphs_dataset(\n",
    "    total_smiles_data=smile_data, \n",
    "    total_pIC50_value=pIC50_data, \n",
    "    )\n",
    "\n",
    "train_smile_data, train_pIC50_data = split_dataset[\"train\"]\n",
    "valid_smile_data, valid_pIC50_data = split_dataset[\"valid\"]\n",
    "infer_smile_data, infer_pIC50_data = split_dataset[\"infer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig = plot_distributions([train_pIC50_data, valid_pIC50_data, infer_pIC50_data], figsize=(10, 3))\n",
    "plt.show()\n",
    "plt.close(hist_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normalise (and later de-normalise) the y-label value:\n",
    "\n",
    "# y_min, y_max = np.min(train_pIC50_data), np.max(train_pIC50_data)\n",
    "# y_min, y_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create pytorch geometric dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"./results/config.json\"))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = prepare_graph_dataset(train_smile_data, train_pIC50_data, problem_type=config[\"problem_type\"])\n",
    "valid_dataset = prepare_graph_dataset(valid_smile_data, valid_pIC50_data, problem_type=config[\"problem_type\"])\n",
    "infer_dataset = prepare_graph_dataset(infer_smile_data, infer_pIC50_data, problem_type=config[\"problem_type\"])\n",
    "len(train_dataset), len(valid_dataset), len(infer_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0], train_dataset[0].x.dtype, train_dataset[0].y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure label distributions are cca. i.i.d.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_fig = plot_distributions([\n",
    "    [g.y.item() for g in train_dataset], \n",
    "    [g.y.item() for g in valid_dataset], \n",
    "    [g.y.item() for g in infer_dataset], \n",
    "], figsize=(10, 3))\n",
    "plt.show()\n",
    "plt.close(hist_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap into the DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_into_dataloader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "valid_loader = load_into_dataloader(valid_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "infer_loader = load_into_dataloader(infer_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the GNN model & train:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss criterion for model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.Tensor([1.0, config[\"pos_class_weight\"]])  # Adjust the value as needed\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "weight, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat_model, gat_optimizer, gat_scheduler = build_model(\n",
    "    num_features=config[\"num_features\"], \n",
    "    embedding_size=config[\"embedding_size\"], \n",
    "    num_heads=config[\"num_attn_heads\"], \n",
    "    dropout_prob=config[\"dropout_prob\"],\n",
    "    use_batch_norm=config[\"use_batch_norm\"],\n",
    "    learning_rate=config[\"learning_rate\"],\n",
    "    weight_decay=config[\"weight_decay\"],\n",
    "    scheduler_gamma=config[\"scheduler_gamma\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model:\n",
    "\n",
    "results = train_model(\n",
    "    gat_model, \n",
    "    loss_fn,\n",
    "    gat_optimizer, \n",
    "    gat_scheduler,    \n",
    "    train_loader = train_loader, \n",
    "    valid_loader = valid_loader, \n",
    "    infer_loader = infer_loader, \n",
    "    num_epochs = config[\"num_epochs\"], \n",
    "    logger_freq = config[\"logging_frequency\"], \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model, train_losses, train_metric, valid_losses, valid_metric, infer_loss, infer_metr = results\n",
    "pre_trained_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fig = plot_training_curves(train_losses, valid_losses, infer_loss, figsize=(5, 3))\n",
    "plt.savefig(\"./results/training_loss.png\")\n",
    "plt.show()\n",
    "plt.close(hist_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fig = plot_training_curves(train_metric, valid_metric, infer_metr, figsize=(5, 3))\n",
    "plt.ylabel(\"Metric [accuracy]\")\n",
    "plt.savefig(\"./results/training_metric.png\")\n",
    "plt.show()\n",
    "plt.close(hist_fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics_and_matrix_regression(data_loader, pre_trained_model, cutoff_limit: float = 8.0):\n",
    "#     y_pred_results = []\n",
    "#     y_true_results = []\n",
    "\n",
    "#     for batch in data_loader:\n",
    "    \n",
    "#         # Passing the node features and the connection info\n",
    "#         predictions, _ = pre_trained_model(batch.x, batch.edge_index, batch.batch)\n",
    "#         y_pred = predictions.squeeze()\n",
    "#         y_true = batch.y.squeeze()\n",
    "        \n",
    "#         # Process into integer predictions:\n",
    "#         y_pred = [value > cutoff_limit for value in y_pred]\n",
    "#         y_true = [value > cutoff_limit for value in y_true]\n",
    "        \n",
    "#         # Store the result:\n",
    "#         y_pred_results.extend(y_pred)\n",
    "#         y_true_results.extend(y_true)\n",
    "\n",
    "#     y_pred_results = np.stack(y_pred_results)\n",
    "#     y_true_results = np.stack(y_true_results)\n",
    "\n",
    "#     accuracy = accuracy_score(y_true=y_true_results, y_pred=y_pred_results)\n",
    "#     precision, recall, f1score, support = precision_recall_fscore_support(y_true=y_true_results, y_pred=y_pred_results, pos_label=1)\n",
    "#     metrics = {\n",
    "#         \"accuracy\" : accuracy, \n",
    "#         \"precision\" : precision, \n",
    "#         \"recall\" : recall, \n",
    "#         \"f1score\" : f1score,\n",
    "# }\n",
    "#     conf_matrix = ConfusionMatrixDisplay.from_predictions(\n",
    "#         y_true=y_true_results, \n",
    "#         y_pred=y_pred_results,\n",
    "#         normalize=\"true\",\n",
    "#         cmap=\"copper\"\n",
    "#     )\n",
    "    \n",
    "#     return metrics, conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, conf_matrix, auroc_curve, avg_pred_curve = compute_metrics_and_matrix_classification(\n",
    "    data_loader=infer_loader, pre_trained_model=pre_trained_model,\n",
    ")\n",
    "\n",
    "print (f\"Inference metrics: {metrics}\")\n",
    "\n",
    "conf_matrix.plot(cmap=\"copper\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"./results/infer_confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "auroc_curve.plot()\n",
    "plt.title(\"Area Under ROC Curve\")\n",
    "plt.savefig(\"./results/infer_area_under_curve.png\")\n",
    "plt.close()\n",
    "\n",
    "avg_pred_curve.plot()\n",
    "plt.title(\"Precision Recall Curve\")\n",
    "plt.savefig(\"./results/infer_precision_recall_curve.png\")\n",
    "plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model & resulting metrics as appropriate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pre_trained_model, \"./results/classifier.pt\")\n",
    "\n",
    "with open(\"./results/metrics.json\", \"w\") as outfile:\n",
    "    json.dump(metrics, outfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
